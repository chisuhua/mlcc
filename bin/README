

https://github.com/ROCm-Developer-Tools/hcc2/bin/README

HCC2 = Heterogenous Compute Compiler Version 2

This is the developer README for HCC2.

This bin directory contains scripts to build HCC2 from source.

clone_hcc2.sh      -  A script to make sure the necessary repos are up to date.
                      See below for a list of these libraries.

build_hcc2.sh      -  Run all build and install scripts for each component

build_roct.sh      -  Build the hsa thunk library

build_rocr.sh      -  Built the ROCm runtime

build_llvm.sh      -  Build llvm, clang, and lld components of HCC2 compiler.
                      This compiler supports openmp, clang hip, clang cuda,
                      opencl, and the GPU  kernel frontend cloc.
                      It contains a recent version of the AMD Lightning compiler
                      (llvm amdgcn backend) and the llvm nvptx backend.
                      This compiler works for both Nvidia and AMD Radeon GPUs.

build_utils.sh     -  Builds the HCC2 utilities
                      This installs in /opt/rocm/hcc2 (or $HCC2)

build_atmi.sh      -  Builds early release of ATMI for hcc2.
                      This installs in /opt/rocm/hcc2 (or $HCC2)

build_hip.sh       -  Builds the hip host runtimes needed by hcc2.
                      This also installs in /opt/rocm/hcc2 (or $HCC2)

build_openmp.sh    -  Builds the OpenMP libraries for hcc2.
                      This also installs in /opt/rocm/hcc2 (or $HCC2)

build_libdevice.sh -  Builds the device bc libraries from rocm-device-libs
                      This also installs in /opt/rocm/hcc2 (or $HCC2)

build_libm.sh      -  Built the libm DBCL (Device BC Library)

HCC2_VERSION_STRING - File to set the version of HCC2 to clone and build . 

The repositories and branches needed by HCC2_0.5-5 currently are:

DIRECTORY NAME *                  HCC2 REPOSITORY **       BRANCH
-------------------------------   ---------------------    ------
$HOME/git/hcc2/hcc2               %rocdev/hcc2             master ***
$HOME/git/hcc2/clang              %rocdev/clang            HCC2-181213 ***
$HOME/git/hcc2/llvm               %rocdev/llvm             HCC2-181213
$HOME/git/hcc2/lld                %rocdev/lld              HCC2-181213
$HOME/git/hcc2/openmp             %rocdev/openmp           HCC2-181213 ***
$HOME/git/hcc2/hip                %rocdev/hip              HCC2.180805
$HOME/git/hcc2/rocm-device-libs   %roc/rocm-device-libs    HCC2-181210
$HOME/git/hcc2/atmi               %roc/atmi                atmi-0.5
$HOME/git/hcc2/openmpapps         %roclib/openmpapps       HCC2-0.5
$HOME/git/hcc2/rocr-runtime       %roclib/rocr-runtime     master
$HOME/git/hcc2/roct-thunk-interfaces  %roclib/roct-thunk-interfaces  master

   * Clone your repositories here or override with environment variables.
  ** Replace %roc with "https://github.com/RadeonOpenCompute"
  ** Replace %rocdev with "https://github.com/ROCm-Developer-Tools"
  ** Replace %roclib with "https://github.com/AMDComputeLibraries"
 *** These are the primary development repositories for HCC2. They are updated often.

The scripts and example makefiles use these environment variables and these 
defaults if they are not set. This is not a complete list.  See the script headers
for other environment variables that you may override including repo names. 

HCC2              /opt/rocm/hcc2           *
CUDA              /usr/local/cuda          *
HCC2_REPOS        /home/$USER/git/hcc2
NVPTXGPUS         30,35,50,60,70           **
BUILD_TYPE        Release
SUDO              set

  * The clang driver uses these environment variables to find device libraries.
 ** The sm_70 (70 in NVPTXGPUS) requires CUDA 9 and above.

If you do not have root access to your machine, you can override the above by setting
the values in your .bashrc or .bash_profile to build your HOME directory.
Here is a sample for your .bash_profile

SUDO="disable"
HCC2=$HOME/install/hcc2
BUILD_TYPE=Debug
NVPTXGPUS=30,35,50,60,70
export SUDO HCC2 NVPTXGPUS BUILD_TYPE

The build scripts will build from the source directories identified by the 
environment variable HCC2_REPOS.

To set alternative installation path for the component INSTALL_<COMPONENT> environment 
variable can be used, e.g. INSTALL_HCC2

To build all components, first clone hcc2 repo and checkout the master branch
to build our development repository.  Checkout a tag such as rel_0.5-1 to build
a released version. 

	git clone https://github.com/ROCm-Developer-Tools/hcc2.git

	git checkout master 

Or to build rel_0.5-1, run this command:

	git checkout rel_0.5-1
	
To be sure you have the latest sources from the git repositories, run command.

        ./clone_hcc2.sh

The first time you do this, It could take a long time to clone the repositories.
Subsequent calls will pull the latest updates.

To build hcc2, you MUST have the Nvidia CUDA SDK version 8 installed because
HCC2 can build  applications for NVIDIA GPUs. We have not done testing for CUDA
version 9.  The current default list of Nvidia subarchs is "30,35,50,60,70".
For example, that will support application builds with --offload-arch=sm_30
and --offload-arch=sm_60 etc.
This can be changed with the NVPTXGPUS environment variable.

You also MUST have ROCm 1.7 installed because HCC2 builds applications for
Radeon GPUs.  The current default list of Radeon GPUS is
"gfx700 gfx701 gfx801 gfx803 gfx900". For example, that will support
application builds with --offload-arch=gfx803 etc.
This can be changed with the GFXLIST environment variable.

After you have all the source repositories and have both cuda and rocm are
installed, run these scripts in the following order:

	./build_roct.sh
	./build_roct.sh install

	./build_rocr.sh
	./build_rocr.sh install

	./build_llvm.sh
	./build_llvm.sh install

	./build_utils.sh
	./build_utils.sh install

	./build_hip.sh
	./build_hip.sh install

	./build_atmi.sh
	./build_atmi.sh install

	./build_openmp.sh  
	./build_openmp.sh install

	./build_libdevice.sh  
	./build_libdevice.sh install

	./build_libm.sh  
	./build_libm.sh install

For now, run this command for some minor fixups to the install.

        ./build_fixup.sh

 ==>  OR run this 1 command that runs each of the above commands:

       ./build_hcc2.sh

Only run build_hcc2.sh if you are confident it will will build without failure.

The first execution of the these scripts does not run "make install" in
case their is a build failure.  In case of a build failure, you can restart
the build by running "make" or "make install" in the build directory. 

Bootstrapping:
The LLVM compiler created by build_llvm.sh is needed by all of the components.
So you must run 'build_llvm.sh' and 'build_llvm.sh' install first before
building any of the components.

The utilities created by build_utils.sh are needed by the 4 library components
build_atmi.sh, build_openmp.sh, build_libdevice.sh and build_hip.sh. 
So you must build and install the utilities before building the libraries.
